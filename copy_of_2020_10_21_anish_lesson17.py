# -*- coding: utf-8 -*-
"""Copy of 2020-10-21_Anish_Lesson17

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-roZxVdyOizb55h2VUxgFz8e43u-qtca

# Lesson 17: Hunting Exoplanets In Space - Model Evaluation

### Teacher-Student Activities

In the previous class, we deployed the `RandomForestClassifier` prediction model which classified the stars, in the test dataset, as `1` and `2`.

The model turned out to be 99% accurate. However, in the dataset, `565` stars are classified as `1` and remaining `5` are classified as `2`. But the Random Forest Classifier model classified every star as `1`. Ideally, it should classify a few stars as `2` because the ultimate goal of the Kepler Space telescope was to find exoplanets in space.

Hence, we need to ensure that for any kind of uneven distribution of data in the test dataset, our model should make accurate predictions. For this purpose, we need to evaluate the model that we deployed.

Generally, a classification model (in this case, Random Forest Classification) is evaluated through a concept called **confusion matrix**.

In this class, we will learn how to evaluate the performance of a classification-based machine learning model using a confusion matrix.


Let's run all the codes in the code cells that we have already covered in the previous classes and begin this class from **Activity 1: The Confusion Matrix** section. You too run the code cells until the first activity.

---

#### Loading The Datasets
Create a Pandas DataFrame every time you start the Jupyter notebook.

Dataset links (don't click on them):

1. Train dataset

  https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTrain.csv

2. Test dataset

  https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTest.csv
"""

# Load the datasets.
import pandas as pd

exo_train_df = pd.read_csv('https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTrain.csv')
exo_test_df = pd.read_csv('https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTest.csv')

"""Check the number of rows and columns in the DataFrames."""

# Number of rows and columns in the DataFrames.
print(exo_train_df.shape)
exo_test_df.shape

"""---

#### The `value_counts()` Function

To compute how many times a value occurs in a series, use the `value_counts()` function.
"""

# The number of times a value occurs in a Pandas series.
exo_test_df['LABEL'].value_counts()

"""There are `565` stars which are classified as `1` and `5` stars classified as `2`. This means that only `5` stars have a planet.

---

#### Importing `RandomForestClassifier` Module
We need to import a module called `RandomForestClassifier` from a package called `sklearn.ensemble`. The `sklearn` or **scikit-learn** is a collection of many machine learning modules. Almost every machine learning algorithm can be directly applied without a knowledge of math using the **scikit-learn** library. It is kind of a plug-and-play device.
"""

# Import the 'RandomForestClassifier' module from the 'sklearn.ensemble' library.
from sklearn.ensemble import RandomForestClassifier

"""---

#### The Target & Feature Variables Separation

The `RandomForestClassifier` module has a function called `fit()` which takes two inputs. The first input is the collection of feature variables. The second input is the target variable. Hence, we need to extract the target variable and the feature variables separately from the training dataset.

Let's store the feature variables in the `x_train` variable and the target variable in the `y_train`.
"""

# Extract feature variables from the training dataset.
x_train = exo_train_df.iloc[:, 1:]
x_train.head()

# Retrieve only the first column, i.e., the 'LABEL' column.
y_train = exo_train_df.iloc[:, 0] 
y_train.head()

"""---

#### Fitting The Model
Let's train the model using the `fit()` function.
"""

# Train the 'RandomForestClassifier' model using the 'fit()' function.
rf_clf = RandomForestClassifier(n_jobs=-1, n_estimators=50)
rf_clf.fit(x_train, y_train)

rf_clf.score(x_train, y_train)

"""As you can see, we have built the Random Forest Classifier model with 50 decision trees. The fitting accuracy score of the model is 100%.

---

#### Target And Feature Variables From Test Dataset
Now we need to make predictions on the test dataset. So, we just need to extract feature variables from the test dataset using the `iloc[]` function.
"""

# Extract the feature variables from the test dataset.
x_test = exo_test_df.iloc[:, 1:]
x_test.head()

"""Let's also extract the target variable from the test dataset so that we can compare the actual target values with the predicted values later."""

# Extract the target variable from the test dataset.
y_test = exo_test_df.iloc[:, 0]
y_test.head()

"""---

#### The `predict()` Function
Now, let's make predictions on the test dataset by calling the `predict()` function with the features variables of the test dataset as an input.
"""

# Make predictions using the 'predict()' function.
y_predicted = rf_clf.predict(x_test)
y_predicted

"""The actual target values are stored in a Pandas series. So, for the sake of consistency, let's convert the NumPy array of the predicted values into a Pandas series."""

# Convert the NumPy array of predicted values into a Pandas series.
y_predicted = pd.Series(y_predicted)
y_predicted.head()

"""Now, let's count the number of stars classified as `1` and `2`."""

# Using the 'value_counts()' function, count the number of times 1 and 2 occur in the predicted values.
y_predicted.value_counts()

"""---

#### Activity 1: The Confusion Matrix
Let's quickly first create a confusion matrix and then will try to understand it.

To create a confusion matrix, first import `confusion_matrix` module from the `sklearn.metrics` library. This library contains all the parameters to evaluate a machine learning model. In addition to the `confusion_matrix` module, let's also import the `classification_report` module. We will use them later to evaluate our module.
"""

# Teacher Action: Import the 'confusion_matrix' and 'classification_report' functions from the 'sklearn.metrics' module.
from sklearn.metrics import confusion_matrix, classification_report

"""Now, create the confusion matrix using the `confusion_matrix()` function. It requires two inputs. The first input is actual target values (`y_test`) and the second input is predicted target values (`y_predicted`)."""

# Teacher Action: Create a confusion matrix using the 'y_test' and 'y_predicted' values.
confusion_matrix(y_test, y_predicted)

"""Now that we have got our confusion matrix, let's try to understand this concept. So, after you deploy the classification model, there are 4 possible outcomes. They are:

1. Class `1` values predicted as class `1`. In this case, 

    - the class `1` values are stars **NOT** having a planet, and

    - the class `2` values are stars having a planet

2. Class `1` values predicted as class `2`.

3. Class `2` values predicted as class `1`.

4. Class `2` values predicted as class `2`.

These 4 possibilities can be reported in a table which is called a confusion matrix. 

||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|
|-|-|-|
|Actual Class `1` (`y_test`)|||
|Actual Class `2` (`y_test`)|||

where

- `y_test` contains the actual class `1` & actual class `2` values

- `y_predicted` contains the predicted class `1` & predicted class `2` values

In this table,

- the values **predicted as class `1` and actually belonging to class `1`** are reported in the first row and first column.

||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|
|-|-|-|
|Actual Class `1` (`y_test`)|565||
|Actual Class `2` (`y_test`)|||

- the values **predicted as class `1` but actually belonging to class `2`** are reported in the second row and first column. 

||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|
|-|-|-|
|Actual Class `1` (`y_test`)|565||
|Actual Class `2` (`y_test`)|5||

- the values **predicted as class `2` and actually belonging to class `2`** are reported in the second row and second column.

||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|
|-|-|-|
|Actual Class `1` (`y_test`)|565||
|Actual Class `2` (`y_test`)|5|0|

- the values **predicted as class `2` but actually belonging to class `1`** are reported in the first row and second column.

||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|
|-|-|-|
|Actual Class `1` (`y_test`)|565|0|
|Actual Class `2` (`y_test`)|5|0|

In this case, the class `1` values refer to the stars not having a planet whereas class `2` values refer to the stars having a planet. 

**Positive Outcome**

Detecting a star having a planet is our desired outcome. In technical terms, the desired outcome is called a *positive outcome*. So, here the positive outcome is the prediction of the stars having a planet, i.e., prediction of the class `2` values. Likewise, finding a star which does not have any planet is a *negative outcome*. So, here the negative outcome is the prediction of the class `1` values. 

Thus, 

- `565` values are **True Negative (TN)** values because they are **truly** predicted as class `1` values.

||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|
|-|-|-|
|Actual Class `1` (`y_test`)|565 (TN)||
|Actual Class `2` (`y_test`)|||


- `5` values are **False Negative (FN)** values because they are **falsely** predicted as class `1` values. They should have been predicted as class `2` values.

||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|
|-|-|-|
|Actual Class `1` (`y_test`)|565 (TN)||
|Actual Class `2` (`y_test`)|5 (FN)||


- `0` values are **True Positive (TP)** values because they are **truly** predicted as class `2` values.

||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|
|-|-|-|
|Actual Class `1` (`y_test`)|565 (TN)||
|Actual Class `2` (`y_test`)|5 (FN)|0 (TP)|


- `0` values are **False Positive (FP)** values because they are **falsely** predicted as class `2` values. They should have been predicted as class `1` values.

||Predicted Class `1` (`y_predicted`)|Predicted Class `2` (`y_predicted`)|
|-|-|-|
|Actual Class `1` (`y_test`)|565 (TN)|0 (FP)|
|Actual Class `2` (`y_test`)|5 (FN)|0 (TP)|

---

#### Activity 2: Precision And Recall^

A good prediction model provides a very large number of true positive (TP) values and a very low number of true negative values.

Now, based on the TP and FP values, we define a parameter called **precision**. It is the ratio of the TP values to the sum of TP and FP values, i.e.,

$$\text{precision} = \frac{\text{TP}}{\text{TP + FP}}$$

Currently, the model has given `0` TP values and `0` FP values. Therefore, the precision value is undefined because

$$\text{precision} = \frac{0}{0 + 0} = \text{undefined}$$

*In mathematics, the division by 0 is undefined (or not defined).*

Based on the TP and FN values, we define another parameter called **recall**. It is the ratio of the TP values to the sum of TP and FN values, i.e, 

$$\text{recall} = \frac{\text{TP}}{\text{TP + FN}}$$

Currently, the model gives `0` TP and `5` FN values. Hence, the recall value is 0 because

$$\text{recall} = \frac{0}{0+5} = \text{0}$$

Imagine if the prediction model labels every star as `2`, i.e, every star has a planet. Then, the number of TP values will be the maximum, i.e., `5` but the number of FP values will also be maximum, i.e., `565`. In such a case, the precision value would be

$$\text{precision} = \frac{5}{5+565} = \frac{5}{570} = 0.008$$

which is very very low.

Also, the model will give `0` FN values. Then, the recall value would be

$$\text{recall} = \frac{5}{5 + 0} = 1$$


So, even though the recall value would be equal to 1, the precision value would be close to 0. Hence, this would be a bad prediction model.


Evidently, there is a trade-off. If the recall value is high, then the precision value will be low and vice-versa. Hence, we need to find an optimum point where both, the precision and the recall values are acceptable.

---

#### Activity 3: The `f1-score`

To find an optimum point where both, the precision and recall values, are high, we calculate another parameter called **f1-score**. It is a harmonic mean of the precision and recall values, i.e.,

$$\text{f1-score} = 2 \left( \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}} \right)$$

So, based on the current predictions, the f1-scores value is undefined because both the precision and recall values are also undefined.

$$\text{f1-score} = 2 \left( \frac{\text{undefined} \times 0}{\text{undefined} + 0} \right) =  \text{undefined}$$

You can also get these values by calling a function called `classification_report()`. It takes two inputs: the actual target values and the predicted target values, i.e., `y_test` and `y_predicted`.

**Note:** You may get the following warning message after executing the code in the code cell below.

```
Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
```

Ignore the warning.
"""

# Student Action: Print the 'precision', 'recall' and 'f1-score' values using the 'classification_report()' function.
print(classification_report(y_test, y_predicted))

"""As you can see, the `precision` and `f1-score` values are reported as `0.00` for class `2` because they are actually undefined values. 
  
Ideally, the above values for class `2` should also be close to `1.00`. Then only we can say that our prediction model is satisfactory. This shows that accuracy alone cannot tell whether a prediction model is making correct predictions or not.

In the next class, we will try to improve the model so that we get the desired precision, recall and f1-score values for class `2`.

**Note:** Now you can attempt **Project 5** on your own.

---
"""