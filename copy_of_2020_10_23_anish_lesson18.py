# -*- coding: utf-8 -*-
"""Copy of 2020-10-23_Anish_Lesson18

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p1efQf3zk9S_228FEvUEQw0uSLAL0mjY

# Lesson 18: Hunting Exoplanets In Space - Data Normalisation

### Teacher-Student Activities

In the last class, we evaluated the Random Forest Classifier model through a concept called the **confusion matrix**. We also calculated the precision, recall and f1-score values. They were undefined. Based on these three parameters, we concluded that our model needs a lot of improvement because it did not classify the stars having a planet as `2` rather it labelled every star as `1`.

In this lesson, we will process the data before deploying a prediction model so that it can learn the properties of the different stars through the training dataset.

Now, there is no right approach to the data processing method. It is an iterative process. It comes through experience and domain knowledge. (*The term 'domain' means a particular field of industry or academics*). For e.g., if you are a banker, then you would have the knowledge of the finance field. Similarly, if you are an astrophysicist, then you would have the technical knowledge of astronomy, quantum mechanics, optics etc. So, based on the knowledge of a respective field, data should be processed before deploying a prediction model.

In this class, we will perform the following data processing exercises:

1. Data Normalisation

2. Fast Fourier Transformation

3. Oversampling

Finally, we will check if the Random Forest Classifier model is still providing the expected results. If not, then we will deploy a much stronger prediction model called XGBoost Classifier. Generally, it doesn't require a lot of data pre-processing but it requires heavy computation resources such as high RAM, CPU, GPU and at least 4 cores processor. Thanks to Google Colab notebook, we have a decent computation power to run the XGBoost Classifier algorithm.

Hence, the XGBoost Classifier model must be used only when the data processing is not helping a prediction model in making accurate predictions.

Let's run all the codes in the code cells that we have already covered in the previous classes and begin this class from the **Activity 1: Data Normalisation** section. You too run the code cells until the first activity.

---

#### Loading The Datasets

Create a Pandas DataFrame every time you start the Jupyter notebook.

Dataset links (don't click on them):

1. Train dataset 

   https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTrain.csv

2. Test dataset 

   https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTest.csv
"""

# Load both the training and test datasets.
import numpy as np
import pandas as pd

exo_train_df = pd.read_csv('https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTrain.csv')
exo_test_df = pd.read_csv('https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTest.csv')

exo_train_df.head()

# The shapes of the 'exo_train_df' and 'exo_test_df' DataFrames.
print(exo_train_df.shape)
exo_test_df.shape

"""In the previous classes, we have already checked the datasets don't have a missing value. So, we can skip that part.

---

#### Activity 1: Data Normalisation^^^

After creating a DataFrame and inspecting data for the missing values, we can normalise the data.

**What is data normalisation?**

Data normalisation is a process of standardising data. It brings every single data-point on a uniform scale. If you look at both the `exo_train_df` and `exo_test_df` DataFrames, they contain highly varying `FLUX` values. 

You can get the data summary using the `describe()` function to look at the variation of the data. 

**Syntax:** `dataframe_name.describe()`
"""

# Teacher Action: Get the data description by calling the 'describe()' function.
exo_train_df.describe()

"""As you can see, the values in the `FLUX.1` column range between `-227,856.3` (minimum `FLUX.1` value) and `1,439,240` (maximum `FLUX.1` value). 

**Note:** `-2.278563e+05` is equivalent to $-2.278563\times10^5$ and `1.439240e+06` equivalent to $1.439240\times10^6$ 

In the `FLUX.1` column, the difference in the maximum and minimum values, i.e., 

$$1,439,240 - (-227,856.3) = 1,667,096.3$$


is huge because of the $10^5$ and $10^6$ scales. 

Similarly, the data in all the other `FLUX` columns also vary a lot because they lie on a huge scale. The big figures are less readable. For e.g., `122` (one hundred twenty two) is more readable than `1,439,240` (one million, four hundred thirty nine thousand, two hundred forty).

The data normalisation process lowers the scale and brings all the data-points on the same scale.

**Why must data be normalised?**

The machine learning models are quite sensitive to the scale of data. They give more importance to the larger values while learning the properties of data. Hence, it becomes crucial for us to remove this bias by bringing down all the data-points on the same scale.


**How to normalise data?**

There are various methods of data normalisation. We will cover all of them throughout this course whenever we need them. For the time being, we will use the *mean normalisation* method. Let's understand the *mean normalisation* method.

Consider a series of numbers having the values 

$$x_1, x_2, x_3, ... , x_N$$ 

where $N$ is the total number of values in a series.

Let 

- $x_{mean}$ denote the mean (or average) value of a series

- $x_{min}$ denote the minimum value in a series and 

- $x_{max}$ denote the maximum value in a series

The normalised value in a series is calculated as

$$x_{norm} = \frac{x_p - x_{mean}}{x_{max} - x_{min}}$$

where 

$$x_p = x_1, x_2, x_3, ..., x_N$$

So after normalisation, the new values in the series will be

$$\left(\frac{x_1 - x_{mean}}{x_{max} - x_{min}}\right), \left(\frac{x_2 - x_{mean}}{x_{max} - x_{min}}\right), \left(\frac{x_3 - x_{mean}}{x_{max} - x_{min}}\right), ..., \left(\frac{x_N - x_{mean}}{x_{max} - x_{min}}\right)$$

For e.g., consider the series $[5, 192, 20019, 12, 209]$.

- The average value of the series is $x_{mean} = 4087.4$

- The minimum value in the series is $x_{min} = 5$

- The maximum value in the series is $x_{max} = 20019$

So, after normalisation, the new series would have the following numbers.

$$\left[ \left( \frac{5 - 4087.4}{20019 - 5} \right), \left( \frac{192 - 4087.4}{20019 - 5} \right), \left( \frac{20019 - 4087.4}{20019 - 5} \right), \left( \frac{12 - 4087.4}{20019 - 5} \right), \left( \frac{209 - 4087.4}{20019 - 5} \right) \right]$$

$$\Rightarrow \left[-0.203977, -0.194634, 0.796023, -0.203627, -0.193784 \right]$$

or 

$$\left[-\frac{203,977}{1,000,000}, -\frac{194,634}{1,000,000}, \frac{796,023}{1,000,000}, -\frac{203,627}{1,000,000}, -
\frac{193,784}{1,000,000} \right]$$

As you can see, after normalisation all the new values are on the same scale which is $\frac{1}{1,000,000}$ or $10^{-6}$.

So, now let's create a function which normalises data in a series. It should take a Pandas series as an input and should return a normalised series as an output.
"""

# Student Action: Create a function to normalise a Pandas series using the mean normalisation method.
def normalization(series):
  normalize_series= (series-series.mean())/(series.max()-series.min())
  return normalize_series

"""Now, let's test the `mean_normalise()` function on the $[5, 192, 20019, 12, 209]$ series. If we get the desired output, then it means the function is working correctly."""

# Student Action: Test the 'mean_normalise()' function on the '[5, 192, 20019, 12, 209]' series.
test_series = pd.Series([5, 192, 20019, 12, 209])
normalization(test_series)

"""Now, let's apply the `mean_normalise()` function on the `exo_train_df` DataFrame to normalise only the `FLUX` values.

Using the `iloc[]` function, we will first exclude the `LABEL` column from the DataFrame and then will apply the `mean_normalise()` function on the `exo_train_df` DataFrame using the `apply()` function.

The `apply()` function takes two inputs. 

- The first input is the function that needs to be applied which in this case is `mean_normalise()` function. 

- The second input is the `axis` value which signifies whether the function in the first input needs to be applied vertically or horizontally.

**Syntax:** `dataframe.apply(function_name, axis)`

**Note:** Whenever you apply a function, say `function_name()` on a DataFrame using the `apply()` function, remove the brackets from the name of the function (i.e., `function_name`) to be applied. That's why the syntax is `dataframe.apply(function_name, axis)`

A DataFrame has two axes (axes is plural of axis). 

- The first axis is the vertical axis. It is represented as `axis = 0` 

- The second axis is the horizontal axis. It is represented as `axis = 1`

The DataFrame axes define whether an operation needs to be applied row-wise or column-wise. Refer to the image shown below.

<img src='https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/lesson-16/dataframe-axes.png' width=600>

1. If `axis = 0`, then it means the function needs to be applied **vertically**. In other words, the function will be applied on all the rows but only **one column at a time.** So, on the `exo_train_df` DataFrame, if the `mean_normalise()` function is applied **vertically**, then it will be applied in the following order:

    - `exo_train_df.iloc[:, 0]`, i.e., all the rows and the `FLUX.1` column at a time.
    
    - `exo_train_df.iloc[:, 1]`, i.e., all the rows and the `FLUX.2` column at a time.

    - `exo_train_df.iloc[:, 2]`, i.e., all the rows and the `FLUX.3` column at a time.

    ...

    - `exo_train_df.iloc[:, 3197]`, i.e., all the rows and the `FLUX.3197` column at a time.

2. If `axis = 1`, then it means the function needs to be applied **horizontally**. This means the function will be applied on all the columns but only **one row at a time.** So, on the `exo_train_df` DataFrame, if the `mean_normalise()` function is applied **horizontally**, then it will be applied in the following order:

    - `exo_train_df.iloc[0, :]`, i.e., the first row and all the columns at a time.
    
    - `exo_train_df.iloc[1, :]`, i.e., the second row and all the columns at a time.

    - `exo_train_df.iloc[2, :]`, i.e., the third row and all the columns at a time.

      ...

    - `exo_train_df.iloc[5086, :]`, i.e., the last row and all the columns at a time.

We will apply the `mean_normalise()` function **horizontally** on the `exo_train_df` DataFrame to normalise the `FLUX` values for a star at a time.
"""

# Teacher Action: Apply the 'mean_normalise' function horizontally on the training DataFrame. 
norm_train_df= exo_train_df.iloc[:,1:].apply(normalization,axis=1)
# After applying the 'mean_normalise' function on the 'exo_train_df' DataFrame, let's print the first 5 rows of the new DataFrame.
norm_train_df.head()

"""You can see that, all the data-points are on the same scale after mean normalisation. Notice that we didn't normalise the `LABEL` data as we intended.

Now, let's insert the `LABEL` column to the `norm_train_df` DataFrame to get the full DataFrame with the normalised `FLUX` values.

We can obtain the `LABEL` column from the `exo_train_df` DataFrame using the `exo_train_df['LABEL']` method.

To insert a column in a DataFrame, use the `insert()` function. It takes three inputs.

- The first input should be the desired column index of the new column after its insertion.

- The second input should be the desired column name.

- The third input should be the values of the new column.

**Syntax:** `dataframe.insert(loc=column_index, column=column_name, value=some_pandas_series)`
"""

# Teacher Action: Apply the 'insert()' function to add the 'LABEL' column to the 'norm_train_df' DataFrame.
norm_train_df.insert(loc=0, column='LABEL', value=exo_train_df['LABEL'])
# After inserting the 'LABEL' column to the 'norm_train_df' DataFrame, print its first five rows.
norm_train_df.head()

"""Now, you normalise the `FLUX` values in the `exo_test_df` DataFrame using the `mean_normalise()` function. Make sure that you apply the function horizontally to normalise the `FLUX` values for a star at a time."""

# Student Action: Apply the 'mean_normalize()' function on the testing DataFrame. Store the new DataFrame in the 'norm_test_df' variable.
norm_test_df = exo_test_df.iloc[:,1:].apply(normalization, axis=1)
norm_test_df.head()
# After applying, the 'mean_normalise()' function on the 'exo_test_df' DataFrame, print the first 5 rows of the new DataFrame.

"""Now, you insert the `LABEL` column to the `norm_test_df` DataFrame with the corresponding `LABEL` values."""

# Student Action: Apply the 'insert()' function to add the 'LABEL' column to the 'norm_test_df' DataFrame.
#norm_test_df.insert(loc=0, column="LABEL", value=exo_test_df["LABEL"])
# After inserting the 'LABEL' column to the 'norm_test_df' DataFrame, print its first five rows.
norm_test_df.head()

"""---

#### Activity 2: Transpose Of A DataFrame^

After the data normalisation, we need to apply Fast Fourier Transformation (which we will learn in the next class). To apply Fast Fourier Transformation, we need to follow three steps.

1. Interchange rows and columns with each other so that columns become rows and rows become columns.

2. Apply the `fft()` function on the DataFrame to apply Fast Fourier Transformation. This we will study in the next class.

3. Interchange rows and columns again.

So, now we will first learn how to interchange the rows and columns. The process of interchanging rows and columns is called **transpose**. It's a very simple process. 

To transpose a DataFrame, you can use the `T` keyword.
"""

# Student Action: Transpose the 'exo_train_df' using the 'T' keyword.
exo_train_df.T

"""As you can see, the rows have become columns and columns have become rows.

In the next class, we will apply Fast Fourier Transformation on the DataFrame.

---
"""