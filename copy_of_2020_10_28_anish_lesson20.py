# -*- coding: utf-8 -*-
"""Copy of 2020-10-28_Anish_Lesson20

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cX51sZv8PSzQEmEXoBo_mJniiP49l9Z7

# Lesson 20: Capstone Class - Hunting Exoplanets In Space

### Teacher-Student Activities

In the previous class, we learnt Fourier transformation, why to apply it on a dataset and how to apply it. 

Also, we transformed both the `exo_train_df` and `exo_test_df` DataFrames by creating the `fast_fourier_transform()` function and then by applying it vertically on the DataFrame using the `apply()` function.

In this class, we will learn how to synthesize (or manufacture) the artificial data points in a dataset by applying an oversampling technique. Generally, in classification problems such as this one, the data is highly imbalanced. 

In a highly imbalanced data, the number of data points for one class is very high compared to another class. The class having the most number of data points is called the **majority class** whereas the class having the least number of data points is called the **minority class**.

In the case of the exoplanets dataset, class `1` is a majority class because the dataset contains the maximum number of stars not having a planet. 

|Class|Training dataset|% Training dataset|Remarks|
|-|-|-|-|
|1|5050|99.273|Majority class|
|2|37|0.727|Minority class|

The `exo_train_df` dataset has a total of `5087` stars in which only `37` stars have a planet and the remaining `5050` stars don't have a planet. The percentage of stars having a planet is $\frac{37 \times 100}{5087} = 0.727 $ % which is very low. Hence, the training dataset is highly imbalanced. 

The test dataset is also highly imbalanced because out of `570` data points, it contains only `5` stars labelled as class `2`. 

|Class|Test dataset|% Test dataset|Remarks|
|-|-|-|-|
|1|565|99.123|Majority class|
|2|5|0.877|Minority class|

So, the percentage of class `2` data points is $\frac{5\times100}{570} = 0.877$ % which is also very low. Thus, the test dataset is also highly imbalanced.

An oversampling technique synthesizes the artificial data points for the minority class data to balance a highly imbalanced dataset. An oversampling technique is required to remove the bias in favour of the majority class in a dataset. 

The major problem with imbalanced data is that a prediction model will always be biased in favour of the majority class in making predictions. Recall that when we deployed the Random Forest Classifier model, it labelled every star in the test dataset as `1` even though the test dataset contains `5` stars belonging to class `2`.

Hence, using an oversampling technique, we can artificially synthesize the minority class data in a training dataset so that both the classes have equal representation in the dataset.

**Note:** The oversampling technique is applied only to the training dataset. It is never applied to the test dataset.

Let's run all the codes in the code cells that we have already covered in the previous classes and begin this class from the **Activity 1: Oversampling For Classification Problems - SMOTE** section. You too run the code cells until the first activity.

---

#### Loading The Datasets

Create a Pandas DataFrame every time you start the Jupyter notebook.

Dataset links (don't click on them):

1. Train dataset

   https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTrain.csv

2. Test dataset
  
   https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTest.csv
"""

# Load both the training and test datasets.
import numpy as np
import pandas as pd

exo_train_df = pd.read_csv('https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTrain.csv')
exo_test_df = pd.read_csv('https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/kepler-exoplanets-dataset/exoTest.csv')

# The shapes of the 'exo_train_df' and 'exo_test_df' DataFrames.
print(exo_train_df.shape)
exo_test_df.shape

"""In the previous classes, we have already checked the datasets don't have a missing value. So, we can skip that part.

---

#### Data Normalisation

After creating a DataFrame and inspecting data for the missing values, we can normalise the data.

$$x_{norm} = \frac{x_p - x_{mean}}{x_{max} - x_{min}}$$
"""

# Function for mean normalisation.
def mean_normalise(series):
  norm_series = (series - series.mean()) / (series.max() - series.min())
  return norm_series

# Applying the 'mean_normalise()' function horizontally on the training DataFrame.
norm_train_df = exo_train_df.iloc[:, 1:].apply(mean_normalise, axis=1)
norm_train_df.head()

# Inserting the 'LABEL' column to the 'norm_train_df' DataFrame.
norm_train_df.insert(loc=0, column='LABEL', value=exo_train_df['LABEL'])
norm_train_df.head()

# Applying the 'mean_normalise()' function on the testing DataFrame. 
norm_test_df = exo_test_df.iloc[:, 1:].apply(mean_normalise, axis=1)
norm_test_df.head()

# Inserting the 'LABEL' column to the 'norm_test_df' DataFrame.
norm_test_df.insert(loc=0, column='LABEL', value=exo_test_df['LABEL'])
norm_test_df.head()

"""---

#### Transpose Of A DataFrame
"""

# Transpose the 'exo_train_df' using the 'T' keyword.
exo_train_df.T

"""---

#### Fast Fourier Transformation

Applying the Fourier Transformation on the datasets.
"""

# Create a function and name it 'fast_fourier_transformation()' to apply Fast Fourier Transformation on the DataFrames.
import numpy as np

def fast_fourier_transform(star):
  fft_star = np.fft.fft(star, n=len(star))
  return np.abs(fft_star)

# Get a frequency array/series for both the training and test datasets.
freq = np.fft.fftfreq(len(exo_train_df.iloc[0, 1:]))
freq

"""This time we will apply the `fast_fourier_transform()` function vertically. So, before applying the function, we will transpose the original DataFrame. Then we will apply the `fast_fourier_transform()` function vertically. Then we will again take the transpose of the DataFrame.

**Note:** We don't want to transform the `LABEL` values. We want to transform the `FLUX` values only.
"""

# Apply the 'fast_fourier_transform()' function on the transposed 'norm_train_df' DataFrame.
x_fft_train_T = norm_train_df.iloc[:, 1:].T.apply(fast_fourier_transform)
x_fft_train = x_fft_train_T.T
x_fft_train.head()

# Applying the 'fast_fourier_transform()' function on the transposed 'norm_test_df' DataFrame.
x_fft_test_T = norm_test_df.iloc[:, 1:].T.apply(fast_fourier_transform)
x_fft_test = x_fft_test_T.T
x_fft_test.head()

"""Our prediction model should be able to recognise the different frequency patterns for different planets and hopefully to classify the stars correctly.

---

#### Activity 1: Oversampling For Classification Problems - SMOTE^

There are 3 different methods to synthesize the artificial data points for a classification problem. They are:

1. Random oversampling

2. SMOTE

3. ADASYN

We will apply the SMOTE method to synthesize the artificial data points in the training dataset. The SMOTE method is the easiest one to understand. The term SMOTE stands for Synthetic Minority Over-Sampling Technique. How the SMOTE technique works, is beyond the scope of this course. But we will learn how to apply it to synthesize the artificial data points for a minority class. 

Before applying the SMOTE method, let's retrieve the `LABEL` data from the training and test DataFrames.
"""

# Student Action: Get the 'y_train' and 'y_test' series from the 'norm_train_df' and 'norm_test_df' DataFrames respectively.
y_train = norm_train_df["LABEL"]
print(y_train)
y_test = norm_test_df["LABEL"]
print(y_test)

"""To apply the `SMOTE` method, we have to follow these steps:

1. From the `imblearn.over_sampling` library import the `SMOTE` module.

2. Then, call the `SMOTE()` function with `ratio=1` as an input. The `ratio=1` denotes that after resampling the dataset, the data points for both the majority and minority class should be in equal numbers. In this case, class `1` has `5050` data points, so class `2` should also have `5050` data points. 

3. Apply the `fit_sample()` function from the `SMOTE` module to synthesize data for the minority class.

**Note:** The `fit_sample()` function returns a NumPy array for both the feature and target variables. Hence, you cannot apply any Pandas series or Python list function on them. You can apply only NumPy functions on them.
"""

# Teacher Action: Apply the 'SMOTE()' function to balance the training data.

# Import the 'SMOTE' module from the 'imblearn.over_sampling' library.
from imblearn.over_sampling import SMOTE
# Call the 'SMOTE()' function with 'ratio=1' as input and store it in the 'sm' variable.
smote= SMOTE(ratio=1)
# Call the 'fit_sample()' function with 'x_fft_train' and 'y_train' datasets as inputs.
x_fft_train_res, y_fft_train_res = smote.fit_sample(x_fft_train,y_train)

"""In the code above, 

1. We are storing the `SMOTE(ratio=1)` function in the `smote` variable.

2. Then, we are generating the artificial values for both the feature and target values using the `fit_sample()` function and then storing them in the `x_fft_train_res` and `y_fft_train_res` variables, respectively.

**Note:** The `fit_sample()` function returns the resampled values in the form of NumPy arrays for both the `x` (feature variable) and `y` (target variable) values.

Let's check the type and shapes of the resampled datasets.
"""

# Student Action: Check the type and shapes of the 'x_fft_train_res' and 'y_fft_train_res' datasets.
print(x_fft_train_res.shape)
print(type(x_fft_train_res))
print(y_fft_train_res.shape)
print(type(y_fft_train_res))

"""As you can see, both the `x_fft_train_res` and `y_fft_train_res` values are stored in the form of NumPy arrays.

Also, we now have `10100` data points for the training dataset containing `5050` class `1` values and `5050` class `2` values.

Let's verify it by using the `sum()` function from the `numpy` module.

---

#### Activity 2: The `sum()` Function^^

The `sum()` function counts the number of times a value occurs in a NumPy array. To apply the `sum()` on a NumPy array, use the following syntax:

**Syntax:** `sum(numpy_array == item)`

where `item` is that item whose number of occurrences in a NumPy array you wish to find.
"""

# Student Action: Find the number of occurrences of class '1' and class '2' values in the 'y_fft_train_res' NumPy array.
print(sum(y_fft_train_res==1))
print(sum(y_fft_train_res==2))

"""As you can see, both the classes, i.e., `1` and `2` appear the equal number of times in the `y_fft_train_res` NumPy array.

Now, let's deploy the Random Forest Classifier prediction model again to see if the prediction model is able to identify the stars having a planet in the test dataset.

---

#### Activity 3: Importing The Required Libraries

Now, import the `RandomForestClassifier` module from the `sklearn.ensemble` library. Also, import the `confusion_matrix` and `classification_report` modules from the `sklearn.metrics` library.
"""

# Student Action: Import the required modules from the 'sklearn.ensemble' and 'sklearn.metrics' libraries.
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

"""---

#### Activity 4: Applying The RandomForestClassifier Model 

Now that we have processed the data to enable our prediction model little more robust, let's once again deploy the Random Forest Classifier model to see if it is able to detect the stars having a planet.
"""

# Student Action: Deploy the random Forest Classifier prediction model.
rlf_cf = RandomForestClassifier(n_jobs=-1, n_estimators=50)
rlf_cf.fit(x_fft_train_res, y_fft_train_res)
print(rlf_cf.score(x_fft_train_res, y_fft_train_res))
y_pred = rlf_cf.predict(x_fft_test)
print(y_pred)

"""Let's quickly make the confusion matrix and classification report to test the efficacy of the model.

---

#### Activity 5: The Confusion Matrix & Classification Report

Now create the confusion matrix and classification report for the model deployed to see if the model is able to detect the stars having a planet.
"""

# Student Action: Create the confusion matrix using the 'y_test' and 'y_pred' values as inputs.
print(confusion_matrix(y_test, y_pred))

"""As you can see, the value in the second row and the second column is `0` which means the Random Forest Classifier model has failed to detect class `2` values. Thus, it failed to detect the stars having a planet.

Hence, this will lead to undefined precision, recall and f1-score values. Let's verify it by printing the classification report.
"""

# Student Action: Print the classification report using the 'y_test' and 'y_pred' values as inputs.
print(classification_report(y_test, y_pred))

"""So, even after processing the data with normalisation, Fast Fourier Transformation and oversampling, the Random Forest Classifier prediction model has failed to detect the stars having a planet. One of the possible reasons for the failure of the Random Forest Classifier model could be its inability to form a right decision tree (recall that random forest is a collection of decision trees). This suggests that maybe we have to further process the data or we might have to apply a different prediction model. 

Let's deploy the **XGBoost Classifier** model to see if it can detect the stars having a planet. If it successfully detects the class `2` values, then it means the XGBoost Classifier model is a more appropriate model here to make prediction compared to the Random Forest Classifier model. If not, then we will have to further process the data and then deploy the classification models again.

---

#### Activity 6: The XGBoost Classifier Model^^^

To deploy the XGBoost Classifier model, we first have to import the `xgboost` library with `xg` as an alias. Then, we need to use the `XGBClassifier()` function to initiate the model. Then, we need to call the `fit()` function with `x_fft_train_res` and `y_fft_train_res` NumPy arrays as input to deploy the model. Finally, we can call the `predict()` function with `x_fft_test` data in the form of NumPy array as an input to get the predicted values.

You can read about the XGBoost Python package by clicking on the link provided in the **Activities** section under the title **XGBoost Python Package**.

**CAUTION:** The XGBoost Classifier is a computationally heavy model. It requires a very high RAM, CPU and GPU to run. It will take some time to learn the feature variables through the training data and then make predictions on the test data. Hence, use it ONLY if all the other lightweight (requiring less RAM, CPU and GPU) prediction models fail.
"""

# Teacher Action: Deploy the XGBoost Classifier model to detect the stars having a planet.
import xgboost as xg
# Call the 'XGBClassifier()' function and store it in the 'model' variable.
model= xg.XGBClassifier()
# Call the 'fit()' function with the 'x_fft_train_res' and 'y_fft_train_res' NumPy arrays as input.
model.fit(x_fft_train_res, y_fft_train_res)
# Make predictions on test data by calling the 'predict()' function with 'x_fft_test' data as input.
y2_pred= model.predict(np.array(x_fft_test))
# Predict the values of predicted values.
print(y2_pred)

"""Now that we have got the predicted values, let's create a confusion matrix to check if the XGBoost Classifier model has detected any class `2` values in the test dataset."""

# Student Action: Create the confusion matrix using the 'y_test' and 'y2_pred' values as inputs.
print(confusion_matrix(y_test, y2_pred))

"""As you can see, the value in the second row and the second column is greater than `0`. Hence, the XGBoost Classifier prediction model has successfully detected few stars belonging to class `2`. Finally, we can take a sigh of relief. However, it has also classified few stars as `1` which should also have been classified as `2`. Nonetheless, this is a great achievement because out of `570` stars in the test dataset, only `5` of them have a planet. And detecting them is like finding a needle in a haystack. So, we should be happy about finding at least 3.


Now, let's compute the precision, recall and f1-scores to test the efficacy of the XGBoost Classifier model. If the f1-score value is greater than 0.5, then we have a good classification model.
"""

# Student Action: Print the classification report using the 'y_test' and 'y2_pred' values as inputs.
print(classification_report(y_test, y2_pred))

"""As you can see, the precision, recall and f1-scores for the class `2` values are quite high. **The closer they are to ONE, the better is the classification model.** 

This is not the best classification model, but it is a fairly good one. So, we don't have to further process the data. The three data processing activities, i.e., mean normalisation, Fourier Transformation and Oversampling are good enough for this problem statement wherein we hunt the exoplanets in space.

---
"""